import cv2
import numpy as np
import json
import os
import sys
from pathlib import Path
import gradio as gr
from insightface.app import FaceAnalysis
from rembg import remove
import ollama

# ---------------------- æœ¬åœ°ç¯å¢ƒé€‚é…ï¼šç‹¬ç«‹é¡¹ç›®ç›®å½•ï¼ˆæ— è·¯å¾„é—®é¢˜ï¼‰ ----------------------
project_dir = Path(__file__).parent / "ChaosPen_æ™ºèƒ½ä¿®å›¾å·¥å…·"
project_dir.mkdir(exist_ok=True)
os.chdir(project_dir)
print(f"âœ… æœ¬åœ°é¡¹ç›®ç›®å½•ï¼š{project_dir.absolute()}")

# ---------------------- åˆå§‹åŒ–äººè„¸æ¨¡å‹ï¼šåŸºç¡€æ£€æµ‹+é®æŒ¡ä¼˜åŒ–åŒæ¨¡å‹ ----------------------
# åŸºç¡€äººåƒè¯†åˆ«æ¨¡å‹ï¼ˆä¼˜å…ˆåˆ¤å®šæ˜¯å¦ä¸ºäººåƒï¼Œé˜ˆå€¼é€‚ä¸­ï¼‰
basic_face_app = None
# é®æŒ¡äººè„¸ä¼˜åŒ–æ¨¡å‹ï¼ˆä»…å½“åˆ¤å®šä¸ºäººåƒåï¼Œå¯åŠ¨è¯¥æ¨¡å‹åšç²¾ç»†åŒ–å¤„ç†ï¼‰
occlusion_face_app = None

try:
    # åŸºç¡€æ¨¡å‹ï¼š640é«˜æ¸…ï¼Œå¸¸è§„äººåƒç²¾å‡†åˆ¤å®š
    basic_face_app = FaceAnalysis(providers=['CPUExecutionProvider'], allowed_modules=['detection'])
    basic_face_app.prepare(ctx_id=0, det_size=(640, 640), threshold=0.5)
    # é®æŒ¡ä¼˜åŒ–æ¨¡å‹ï¼š320è½»é‡ï¼Œä½é˜ˆå€¼é€‚é…é®æŒ¡/åŠè„¸/ä¾§è„¸
    occlusion_face_app = FaceAnalysis(providers=['CPUExecutionProvider'], allowed_modules=['detection'])
    occlusion_face_app.prepare(ctx_id=0, det_size=(320, 320), threshold=0.4)
    print("âœ… äººè„¸æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆåŸºç¡€è¯†åˆ«+é®æŒ¡ä¼˜åŒ–åŒæ¨¡å¼ï¼‰")
except Exception as e:
    print(f"âš ï¸  Insightfaceæ¨¡å‹å…œåº•ï¼š{str(e)[:60]}...")
    print("âœ… è‡ªåŠ¨å¯ç”¨OpenCV Haaräººè„¸æ£€æµ‹ï¼ˆåŸºç¡€+é®æŒ¡åŒæµç¨‹ï¼‰")

# å›ºå®šé…ç½®ï¼šæ”¯æŒçš„ä¿®å›¾æ“ä½œ/æ»¤é•œ
SUPPORT_OPERATIONS = ["ç¾é¢œ", "ç£¨çš®", "ç¾ç™½", "æ¸…æ™°", "æ”¾å¤§", "æŠ å›¾", "å»èƒŒæ™¯"]
SUPPORT_FILTERS = ["ç”µå½±æ„Ÿ", "æ¸…æ–°æ—¥ç³»", "å¤å¤èƒ¶ç‰‡", "é»‘é‡‘è´¨æ„Ÿ", "èµ›åšæœ‹å…‹", "æ°´å¢¨é£"]

# ---------------------- ç¬¬ä¸€æ­¥ï¼šåŸºç¡€äººåƒè¯†åˆ«ï¼ˆæ ¸å¿ƒåˆ¤å®šï¼Œå…ˆç¡®å®šæ˜¯å¦ä¸ºäººåƒï¼‰ ----------------------
def basic_face_detection(img):
    """åŸºç¡€äººåƒæ£€æµ‹ï¼Œå…ˆåˆ¤å®šæ˜¯å¦ä¸ºäººåƒï¼Œè¿”å›True/False"""
    if img is None or len(img.shape) != 3 or img.shape[0] < 30 or img.shape[1] < 30:
        return False
    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    # æ–¹æ¡ˆ1ï¼šInsightfaceåŸºç¡€æ¨¡å‹ï¼ˆä¼˜å…ˆï¼‰
    if basic_face_app is not None:
        try:
            faces = basic_face_app.get(img_bgr, max_num=1)
            if len(faces) > 0:
                return True
        except:
            pass
    # æ–¹æ¡ˆ2ï¼šOpenCV HaaråŸºç¡€æ£€æµ‹ï¼ˆå…œåº•ï¼‰
    try:
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces_cv = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=3, minSize=(30, 30))
        if len(faces_cv) > 0:
            return True
    except Exception as e:
        pass
    return False

# ---------------------- ç¬¬äºŒæ­¥ï¼šé®æŒ¡äººè„¸ä¸“é¡¹ä¼˜åŒ–ï¼ˆä»…å½“åŸºç¡€è¯†åˆ«ä¸ºäººåƒåï¼Œæ‰æ‰§è¡Œæ­¤æ­¥éª¤ï¼‰ ----------------------
def occlusion_face_optimization(img):
    """é®æŒ¡äººè„¸ç²¾ç»†åŒ–å¤„ç†ï¼Œä»…å¯¹å·²åˆ¤å®šçš„äººåƒåšé®æŒ¡/åŠè„¸/ä¾§è„¸ä¼˜åŒ–ï¼Œè¿”å›å¤„ç†åçš„å›¾åƒ"""
    if img is None or len(img.shape) != 3 or img.shape[0] < 30 or img.shape[1] < 30:
        return img
    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    img_copy = img_bgr.copy()
    # æ–¹æ¡ˆ1ï¼šInsightfaceé®æŒ¡ä¼˜åŒ–æ¨¡å‹ï¼ˆä¼˜å…ˆï¼Œé€‚é…é®æŒ¡/åŠè„¸/ä¾§è„¸ï¼‰
    if occlusion_face_app is not None:
        try:
            faces = occlusion_face_app.get(img_bgr, max_num=1)
            if len(faces) > 0:
                return cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)
        except:
            pass
    # æ–¹æ¡ˆ2ï¼šOpenCV Haaré®æŒ¡ä¸“é¡¹æ£€æµ‹ï¼ˆå…œåº•ï¼Œå«ä¾§è„¸/åŠè„¸/å£ç½©é®æŒ¡ï¼‰
    try:
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        # é®æŒ¡/åŠè„¸æ£€æµ‹ï¼šä½é‚»åŸŸ+ç²¾ç»†ç¼©æ”¾
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces_occlusion = face_cascade.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=2, minSize=(25, 25))
        # ä¾§è„¸æ£€æµ‹è¡¥å……
        profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')
        faces_profile = profile_cascade.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=2, minSize=(25, 25))
        if len(faces_occlusion) > 0 or len(faces_profile) > 0:
            return cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)
    except Exception as e:
        pass
    return cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)

# ---------------------- åœºæ™¯æ€»è¯†åˆ«ï¼šå…ˆåŸºç¡€äººè„¸â†’å†åˆ¤åŠ¨ç‰©/æ™¯ç‰©ï¼ˆä¸¥æ ¼æŒ‰ä¼˜å…ˆçº§ï¼‰ ----------------------
def detect_scene_by_image(img):
    """
    åœºæ™¯è¯†åˆ«æ€»æµç¨‹ï¼š
    1. å…ˆæ‰§è¡Œbasic_face_detectionï¼Œåˆ¤å®šä¸ºäººåƒåˆ™è¿”å›ã€Œäººåƒã€
    2. éäººåƒæ—¶ï¼Œå†æ‰§è¡ŒåŠ¨ç‰©/æ™¯ç‰©åˆ¤å®š
    """
    if img is None or len(img.shape) not in [2, 3] or img.shape[0] < 20 or img.shape[1] < 20:
        return "æ™¯ç‰©"
    # ç¬¬ä¸€æ­¥ï¼šå…ˆåŸºç¡€äººåƒè¯†åˆ«ï¼Œåˆ¤å®šä¸ºäººåƒåˆ™ç›´æ¥è¿”å›ï¼Œåç»­å†åšé®æŒ¡ä¼˜åŒ–
    if basic_face_detection(img):
        return "äººåƒ"
    # éäººåƒï¼šå†åˆ¤å®šåŠ¨ç‰©/æ™¯ç‰©
    if len(img.shape) == 3:
        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
        lower_warm = np.array([0, 20, 50])
        upper_warm = np.array([40, 255, 255])
        mask_warm = cv2.inRange(hsv, lower_warm, upper_warm)
        warm_ratio = np.sum(mask_warm) / (img.shape[0] * img.shape[1]) if (img.shape[0] * img.shape[1]) > 0 else 0
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        edge = cv2.Canny(gray, 50, 150)
        texture_ratio = np.sum(edge) / (img.shape[0] * img.shape[1] * 255) if (img.shape[0] * img.shape[1] * 255) > 0 else 0
        contours, _ = cv2.findContours(edge, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        circularity = 0
        if contours:
            max_contour = max(contours, key=cv2.contourArea)
            perimeter = cv2.arcLength(max_contour, True)
            area = cv2.contourArea(max_contour)
            if perimeter > 0:
                circularity = 4 * np.pi * area / (perimeter ** 2)
        # åŠ¨ç‰©ä¸‰é‡åˆ¤å®šæ¡ä»¶ï¼ˆç¼ºä¸€ä¸å¯ï¼Œæœç»æ™¯ç‰©è¯¯åˆ¤ï¼‰
        if warm_ratio > 0.15 and (0.08 < texture_ratio < 0.35) and circularity > 0.2:
            return "åŠ¨ç‰©"
    # éäººè„¸+éåŠ¨ç‰© â†’ æ™¯ç‰©
    return "æ™¯ç‰©"

# ---------------------- åˆ†åœºæ™¯å¢å¼ºï¼šæ™¯ç‰©è¶…è½»å¾®ä¼˜åŒ–ï¼Œäººåƒ/åŠ¨ç‰©æ­£å¸¸å¢å¼º ----------------------
def enhance_blurry_image(img, scene="æ™¯ç‰©"):
    if img is None or len(img.shape) not in [2, 3] or img.shape[0] < 20 or img.shape[1] < 20:
        return img
    # æ™¯ç‰©åœºæ™¯ï¼šè¶…è½»é‡å»å™ª+é”åŒ–ï¼Œä¸¥æ ¼ä¿ç•™åŸå›¾è´¨æ„Ÿï¼Œå‡ ä¹æ— è§†è§‰å˜åŒ–
    if scene == "æ™¯ç‰©":
        img_denoise = cv2.fastNlMeansDenoisingColored(img, None, 1, 1, 7, 21) if len(img.shape)==3 else img
        kernel = np.array([[0, -0.05, 0], [-0.05, 1.1, -0.05], [0, -0.05, 0]])  # æè‡´è½»é‡é”åŒ–
    # äººåƒ/åŠ¨ç‰©åœºæ™¯ï¼šæ­£å¸¸ä¼˜åŒ–å¹…åº¦ï¼Œä¿è¯æ¸…æ™°æ•ˆæœ
    else:
        is_low_res = img.shape[0] < 200 or img.shape[1] < 200
        kernel = np.array([[0, -0.3, 0], [-0.3, 2.2, -0.3], [0, -0.3, 0]]) if is_low_res else np.array([[0, -0.5, 0], [-0.5, 3, -0.5], [0, -0.5, 0]])
        img_denoise = cv2.fastNlMeansDenoisingColored(img, None, 8, 8, 7, 21) if (len(img.shape)==3 and not is_low_res) else img
    img_sharpen = cv2.filter2D(img_denoise, -1, kernel)
    return img_sharpen

# ---------------------- äººåƒä¸“å±é»„é‡‘æ¯”ä¾‹ä¼˜åŒ–ï¼ˆå«é®æŒ¡äººåƒé€‚é…ï¼Œè‡ªç„¶ä¸å‡ï¼‰ ----------------------
def optimize_face_with_golden_ratio(img):
    if img is None or len(img.shape) != 3 or img.shape[0] < 50 or img.shape[1] < 50:
        return img
    # å…ˆæ‰§è¡Œé®æŒ¡äººè„¸ä¼˜åŒ–ï¼Œå†åšç¾é¢œå¤„ç†
    img_occlusion_optim = occlusion_face_optimization(img)
    img_copy = cv2.cvtColor(img_occlusion_optim, cv2.COLOR_RGB2BGR)
    GOLDEN_RATIO = 1.618
    ADJUST_SCALE = 0.25
    MIN_SCALE = 1 - ADJUST_SCALE
    MAX_SCALE = 1 + ADJUST_SCALE
    ANGLE_THRESH = 15
    MIN_REGION_SIZE = 15
    # ä¼˜å…ˆç”¨é®æŒ¡ä¼˜åŒ–æ¨¡å‹æ£€æµ‹å…³é”®ç‚¹
    faces = occlusion_face_app.get(img_copy, max_num=1) if occlusion_face_app is not None else []
    if not faces:
        # æ— å…³é”®ç‚¹æ—¶ï¼Œä»…åšåŸºç¡€ç£¨çš®ç¾ç™½ï¼ˆé€‚é…é®æŒ¡/åŠè„¸ï¼‰
        gray = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)
        contrast = gray.std()
        d = 8 if contrast < 50 else 12
        sigmaColor = 40 if contrast < 50 else 60
        sigmaSpace = 40 if contrast < 50 else 60
        img_copy = cv2.bilateralFilter(img_copy, d, sigmaColor, sigmaSpace)
        alpha = 1.05 if contrast < 50 else 1.1
        beta = 3 if contrast < 50 else 5
        img_copy = cv2.addWeighted(img_copy, alpha, np.zeros_like(img_copy), 0, beta)
        return cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)
    # æœ‰å…³é”®ç‚¹æ—¶ï¼Œé»„é‡‘æ¯”ä¾‹äº”å®˜å¾®è°ƒï¼ˆä»…å¯¹å¯è§éƒ¨ä½å¤„ç†ï¼‰
    for face in faces:
        kps = face.get('kps', None)
        if kps is None or len(kps) < 15:
            continue
        # çœ¼ç›åŒºåŸŸä¼˜åŒ–ï¼ˆä»…å¤„ç†å¯è§çœ¼ç›ï¼‰
        eye_left, eye_right = kps[0], kps[1]
        eye_vector = eye_right - eye_left
        angle = np.degrees(np.arctan2(eye_vector[1], eye_vector[0]))
        if abs(angle) >= ANGLE_THRESH:
            continue
        eye_width = np.linalg.norm(eye_right - eye_left)
        y1, y2 = max(0, int(eye_left[1]-eye_width/2)), min(img_copy.shape[0], int(eye_right[1]+eye_width/2))
        x1, x2 = max(0, int(eye_left[0]-eye_width/2)), min(img_copy.shape[1], int(eye_right[0]+eye_width/2))
        if (x2 - x1) > MIN_REGION_SIZE and (y2 - y1) > MIN_REGION_SIZE:
            eye_region = img_copy[y1:y2, x1:x2]
            if eye_region.size > 0:
                eye_region = cv2.resize(eye_region, None, fx=MAX_SCALE, fy=MAX_SCALE, interpolation=cv2.INTER_CUBIC)
                eye_region = cv2.resize(eye_region, (x2-x1, y2-y1), interpolation=cv2.INTER_CUBIC)
                img_copy[y1:y2, x1:x2] = eye_region
        # ä¸‹å·´/è„¸é¢Šç²¾ç»†åŒ–å¾®è°ƒï¼ˆé€‚é…é®æŒ¡åçš„é¢éƒ¨è½®å»“ï¼‰
        nose, chin = kps[2], kps[8]
        face_height = np.linalg.norm(chin - nose)
        y1, y2 = max(0, int(nose[1])), min(img_copy.shape[0], int(chin[1]+face_height/4))
        x1, x2 = max(0, int(nose[0]-face_height/2)), min(img_copy.shape[1], int(nose[0]+face_height/2))
        if (x2 - x1) > MIN_REGION_SIZE and (y2 - y1) > MIN_REGION_SIZE:
            chin_region = img_copy[y1:y2, x1:x2]
            if chin_region.size > 0:
                chin_region = cv2.resize(chin_region, None, fx=MIN_SCALE, fy=1, interpolation=cv2.INTER_CUBIC)
                chin_region = cv2.resize(chin_region, (x2-x1, y2-y1), interpolation=cv2.INTER_CUBIC)
                img_copy[y1:y2, x1:x2] = chin_region
        # åŠ¨æ€ç£¨çš®ç¾ç™½ï¼ˆæ ¹æ®é¢éƒ¨å¯è§åº¦é€‚é…ï¼Œä¸å‡ç™½ï¼‰
        gray = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)
        contrast = gray.std()
        d = 8 if contrast < 50 else 12
        sigmaColor = 40 if contrast < 50 else 60
        sigmaSpace = 40 if contrast < 50 else 60
        img_copy = cv2.bilateralFilter(img_copy, d, sigmaColor, sigmaSpace)
        alpha = 1.05 if contrast < 50 else 1.1
        beta = 3 if contrast < 50 else 5
        img_copy = cv2.addWeighted(img_copy, alpha, np.zeros_like(img_copy), 0, beta)
    return cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)

# ---------------------- æ»¤é•œåº”ç”¨ï¼šåˆ†åœºæ™¯é€‚é…ï¼Œä½æ¸…å›¾è‡ªåŠ¨é™æ•ˆæœ ----------------------
def apply_filter(img, filter_name):
    if img is None or filter_name not in SUPPORT_FILTERS or len(img.shape) != 3 or img.shape[0] < 20 or img.shape[1] < 20:
        return img
    img_copy = img.copy()
    rows, cols = img_copy.shape[:2]
    is_low_res = rows < 200 or cols < 200  # ä½æ¸…å›¾è‡ªåŠ¨é™ä½æ»¤é•œå¼ºåº¦ï¼Œé¿å…å¤±çœŸ
    if filter_name == "ç”µå½±æ„Ÿ":
        alpha = 1.2 if is_low_res else 1.3
        beta = -15 if is_low_res else -20
        img_copy = cv2.addWeighted(img_copy, alpha, np.zeros_like(img_copy), 0, beta)
        b, g, r = cv2.split(img_copy)
        b = cv2.addWeighted(b, 1.1 if is_low_res else 1.15, np.zeros_like(b), 0, 0)
        r = cv2.addWeighted(r, 1.03 if is_low_res else 1.05, np.zeros_like(r), 0, 0)
        img_copy = cv2.merge((b, g, r))
        mask = cv2.getGaussianKernel(cols, 200 if is_low_res else 300) @ cv2.getGaussianKernel(rows, 200 if is_low_res else 300).T
        mask = cv2.resize(mask, (cols, rows))
        mask = np.stack([mask]*3, axis=-1)
        img_copy = (img_copy * (mask / mask.max())).astype(np.uint8)
    elif filter_name == "æ¸…æ–°æ—¥ç³»":
        alpha = 1.03 if is_low_res else 1.05
        beta = 10 if is_low_res else 15
        img_copy = cv2.addWeighted(img_copy, alpha, np.zeros_like(img_copy), 0, beta)
        hsv = cv2.cvtColor(img_copy, cv2.COLOR_RGB2HSV)
        hsv[:, :, 1] = cv2.addWeighted(hsv[:, :, 1], 0.85 if is_low_res else 0.75, np.zeros_like(hsv[:, :, 1]), 0, 0)
        img_copy = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
        b = cv2.split(img_copy)[0]
        b = cv2.addWeighted(b, 1.03 if is_low_res else 1.05, np.zeros_like(b), 0, 0)
        img_copy = cv2.merge((b, cv2.split(img_copy)[1], cv2.split(img_copy)[2]))
    elif filter_name == "å¤å¤èƒ¶ç‰‡":
        img_copy = cv2.addWeighted(img_copy, 0.95, np.zeros_like(img_copy), 0, 20)
        b, g, r = cv2.split(img_copy)
        r = cv2.addWeighted(r, 1.1 if is_low_res else 1.15, np.zeros_like(r), 0, 0)
        g = cv2.addWeighted(g, 1.03 if is_low_res else 1.05, np.zeros_like(g), 0, 0)
        img_copy = cv2.merge((b, g, r))
        noise = np.random.normal(0, 1 if is_low_res else 3, img_copy.shape).astype(np.int16)
        img_copy = np.clip(img_copy + noise, 0, 255).astype(np.uint8)
    elif filter_name == "é»‘é‡‘è´¨æ„Ÿ":
        hls = cv2.cvtColor(img_copy, cv2.COLOR_RGB2HLS)
        thresh = 140 if is_low_res else 130
        hls[:, :, 1] = cv2.threshold(hls[:, :, 1], thresh, 255, cv2.THRESH_BINARY)[1]
        img_copy = cv2.cvtColor(hls, cv2.COLOR_HLS2RGB)
        alpha = 1.3 if is_low_res else 1.4
        beta = -10 if is_low_res else -15
        img_copy = cv2.addWeighted(img_copy, alpha, np.zeros_like(img_copy), 0, beta)
    elif filter_name == "èµ›åšæœ‹å…‹":
        hsv = cv2.cvtColor(img_copy, cv2.COLOR_RGB2HSV)
        hsv[:, :, 1] = cv2.addWeighted(hsv[:, :, 1], 1.2 if is_low_res else 1.4, np.zeros_like(hsv[:, :, 1]), 0, 0)
        hsv[:, :, 2] = cv2.addWeighted(hsv[:, :, 2], 1.05 if is_low_res else 1.1, np.zeros_like(hsv[:, :, 2]), 0, 0)
        img_copy = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
        b, r = cv2.split(img_copy)[0], cv2.split(img_copy)[2]
        b = cv2.addWeighted(b, 1.15 if is_low_res else 1.25, np.zeros_like(b), 0, 0)
        r = cv2.addWeighted(r, 1.15 if is_low_res else 1.25, np.zeros_like(r), 0, 0)
        img_copy = cv2.merge((b, cv2.split(img_copy)[1], r))
    elif filter_name == "æ°´å¢¨é£":
        gray = cv2.cvtColor(img_copy, cv2.COLOR_RGB2GRAY)
        min_val = 40 if is_low_res else 60
        max_val = 140 if is_low_res else 160
        edge = cv2.Canny(gray, min_val, max_val)
        edge = cv2.cvtColor(edge, cv2.COLOR_GRAY2RGB)
        gray_rgb = gray[:, :, np.newaxis].repeat(3, axis=2)
        img_copy = cv2.addWeighted(gray_rgb, 0.8, edge, 0.2, 0)
    return img_copy

# ---------------------- AIæŒ‡ä»¤è§£æï¼ˆé€‚é…Llama3ï¼Œæœ¬åœ°è¿è¡Œï¼‰ ----------------------
def parse_prompt_by_llm(user_prompt):
    if not user_prompt or not user_prompt.strip():
        return {"operations": [], "filter": ""}
    system_prompt = f"""
    ä½ æ˜¯å›¾åƒç¼–è¾‘æŒ‡ä»¤è§£æä¸“å®¶ï¼Œä»…æå–æ“ä½œå’Œæ»¤é•œï¼Œè¾“å‡ºä¸¥æ ¼JSONæ ¼å¼ï¼ˆæ— å¤šä½™æ–‡å­—ï¼‰ã€‚
    è§„åˆ™ï¼š1.æ“ä½œä»…ä»[{','.join(SUPPORT_OPERATIONS)}]é€‰ï¼Œæ— åˆ™[]ï¼›2.æ»¤é•œä»…ä»[{','.join(SUPPORT_FILTERS)}]é€‰ï¼Œæ— åˆ™""ï¼›
    3.æ¨¡ç³ŠæŒ‡ä»¤ï¼ˆå¦‚På¥½çœ‹/è°ƒæ¸…æ™°ï¼‰ä»…æå–["æ¸…æ™°"]ï¼Œè¾“å‡ºå›ºå®šé”®ï¼šoperationsã€filterã€‚
    """
    try:
        response = ollama.chat(
            model="llama3",
            messages=[{"role":"system","content":system_prompt.strip()},{"role":"user","content":user_prompt.strip()}]
        )
        result = json.loads(response['message']['content'].strip())
        result["operations"] = [op for op in result.get("operations", []) if op in SUPPORT_OPERATIONS]
        result["filter"] = result.get("filter", "") if result.get("filter") in SUPPORT_FILTERS else ""
        return result
    except Exception as e:
        return {"operations": ["æ¸…æ™°"], "filter": ""}

# ---------------------- ä¸»å¤„ç†å‡½æ•°ï¼ˆä¸¥æ ¼æŒ‰ï¼šåŸºç¡€äººè„¸â†’é®æŒ¡ä¼˜åŒ–â†’åˆ†åœºæ™¯å¤„ç†ï¼‰ ----------------------
def process_image(input_img, user_prompt):
    if input_img is None:
        return None, "âš ï¸ è¯·å…ˆä¸Šä¼ æœ‰æ•ˆå›¾ç‰‡ï¼ˆå»ºè®®â‰¥200*200ï¼‰ï¼"
    # 1. åœºæ™¯æ£€æµ‹ï¼ˆå…ˆåŸºç¡€äººè„¸â†’å†åˆ¤åŠ¨ç‰©/æ™¯ç‰©ï¼‰
    scene = detect_scene_by_image(input_img)
    # 2. è§£æç”¨æˆ·æŒ‡ä»¤
    prompt_result = parse_prompt_by_llm(user_prompt)
    operations = prompt_result["operations"]
    filter_name = prompt_result["filter"]
    # 3. éäººåƒè¿‡æ»¤ç¾é¢œæ“ä½œï¼Œé¿å…è¯¯å¤„ç†
    if scene != "äººåƒ":
        operations = [op for op in operations if op not in ["ç¾é¢œ", "ç£¨çš®", "ç¾ç™½"]]
    # 4. æ— æŒ‡å®šæ»¤é•œï¼ŒæŒ‰åœºæ™¯è‡ªåŠ¨åŒ¹é…
    if not filter_name:
        filter_name = "æ¸…æ–°æ—¥ç³»" if scene == "åŠ¨ç‰©" else "ç”µå½±æ„Ÿ"
    # 5. åˆå§‹åŒ–ç»“æœå’Œæ—¥å¿—
    img_result = input_img.copy()
    msg_list = [f"âœ… åœºæ™¯æ£€æµ‹ç»“æœï¼š{scene}ï¼ˆå…ˆåŸºç¡€è¯†åˆ«â†’å†ç²¾ç»†åŒ–å¤„ç†ï¼‰"]
    # 6. æ— æ“ä½œæ—¶é»˜è®¤æ‰§è¡Œæ¸…æ™°ä¼˜åŒ–
    if not operations:
        operations = ["æ¸…æ™°"]
        msg_list.append("â„¹ï¸  æœªæå–ä¿®å›¾æ“ä½œï¼Œé»˜è®¤æ‰§è¡Œåœºæ™¯ä¸“å±æ¸…æ™°ä¼˜åŒ–")
    # 7. æ ¸å¿ƒå¤„ç†æµç¨‹ï¼ˆæŒ‰ä¼˜å…ˆçº§æ‰§è¡Œï¼‰
    for op in operations:
        if op == "æ¸…æ™°" and img_result is not None:
            img_result = enhance_blurry_image(img_result, scene)
            msg_list.append(f"âœ… æ‰§è¡Œ{scene}ä¸“å±æ¸…æ™°ä¼˜åŒ–ï¼ˆç²¾ç»†å¹…åº¦ï¼Œä¿ç•™åŸå›¾è´¨æ„Ÿï¼‰")
        elif op == "æ”¾å¤§" and img_result is not None:
            img_result = cv2.resize(img_result, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)
            img_result = enhance_blurry_image(img_result, scene)
            msg_list.append(f"âœ… 2å€é«˜æ¸…æ”¾å¤§+{scene}ä¸“å±é”åŒ–")
        elif op in ["æŠ å›¾", "å»èƒŒæ™¯"] and img_result is not None:
            try:
                img_result = remove(img_result)
                msg_list.append(f"âœ… æˆåŠŸ{op}ï¼Œä¿ç•™ä¸»ä½“ç»†èŠ‚")
            except Exception as e:
                msg_list.append(f"âš ï¸  {op}å¤±è´¥ï¼Œå·²è·³è¿‡è¯¥æ“ä½œ")
    # 8. äººåƒä¸“å±å¤„ç†ï¼šå…ˆé®æŒ¡ä¼˜åŒ–â†’å†ç¾é¢œ/ç£¨çš®/ç¾ç™½ï¼ˆæ ¸å¿ƒä¼˜å…ˆçº§ï¼‰
    if scene == "äººåƒ" and img_result is not None and len(img_result.shape)==3:
        msg_list.append("âœ… å¯åŠ¨äººåƒç²¾ç»†åŒ–å¤„ç†ï¼šå…ˆé®æŒ¡ä¼˜åŒ–â†’å†ç¾é¢œæ“ä½œ")
        # å…ˆæ‰§è¡Œé®æŒ¡äººè„¸ä¼˜åŒ–
        img_result = occlusion_face_optimization(img_result)
        # å†æ‰§è¡Œç¾é¢œ/ç£¨çš®/ç¾ç™½
        if "ç¾é¢œ" in operations:
            img_result = optimize_face_with_golden_ratio(img_result)
            msg_list.append("âœ… æ‰§è¡Œäººåƒé»„é‡‘æ¯”ä¾‹ç¾é¢œï¼ˆé€‚é…é®æŒ¡/åŠè„¸ï¼Œè‡ªç„¶ä¸å‡ï¼‰")
        elif "ç£¨çš®" in operations or "ç¾ç™½" in operations:
            gray = cv2.cvtColor(img_result, cv2.COLOR_RGB2GRAY)
            contrast = gray.std()
            d = 8 if contrast < 50 else 12
            img_result = cv2.bilateralFilter(img_result, d, 40 if contrast < 50 else 60, 40 if contrast < 50 else 60)
            alpha = 1.05 if contrast < 50 else 1.1
            beta = 3 if contrast < 50 else 5
            img_result = cv2.addWeighted(img_result, alpha, np.zeros_like(img_result), 0, beta)
            msg_list.append(f"âœ… æ‰§è¡Œäººåƒ{','.join([op for op in operations if op in ['ç£¨çš®','ç¾ç™½']])}ï¼ˆåŠ¨æ€å‚æ•°ï¼Œä¸å‡ç™½ï¼‰")
    # åŠ¨ç‰©/æ™¯ç‰©æ‰§è¡Œä¸“å±æ»¤é•œ
    if scene in ["åŠ¨ç‰©", "æ™¯ç‰©"] and img_result is not None and len(img_result.shape)==3:
        img_result = apply_filter(img_result, filter_name)
        msg_list.append(f"âœ… æ‰§è¡Œ{scene}ä¸“å±ã€Œ{filter_name}ã€æ»¤é•œï¼ˆé€‚é…åœºæ™¯ç‰¹å¾ï¼Œæ•ˆæœè‡ªç„¶ï¼‰")
    # å¼‚å¸¸å…œåº•ï¼Œè¿”å›åŸå›¾
    if img_result is None:
        img_result = input_img
        msg_list.append("âš ï¸  å¤„ç†å¼‚å¸¸ï¼Œå·²è¿”å›åŸå§‹å›¾ç‰‡")
    # æ‹¼æ¥æ—¥å¿—
    result_msg = "\n".join(msg_list)
    return img_result, result_msg

# ---------------------- Gradioå¯è§†åŒ–ç•Œé¢ï¼ˆæœ¬åœ°å‹å¥½ï¼Œæ“ä½œç®€æ´ï¼‰ ----------------------
if __name__ == "__main__":
    with gr.Blocks(title="æ··æ²Œç”»ç¬” - æœ¬åœ°æ™ºèƒ½ä¿®å›¾", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¨ æ··æ²Œç”»ç¬”")
        gr.Markdown("### ğŸ” å…ˆäººåƒè¯†åˆ«â†’å†é®æŒ¡ä¼˜åŒ– | ğŸ–¼ï¸ åˆ†åœºæ™¯ç²¾å‡†ä¿®å›¾ | ğŸ“¸ è‡ªç„¶è¯­è¨€æŒ‡ä»¤")
        gr.Markdown("#### âœ¨ æœ¬åœ°è¿è¡Œç‰ˆ | æ— ç½‘ç»œä¾èµ– | éšç§ä¿æŠ¤")
        gr.Markdown("#### ğŸ“Œ æ”¯æŒæŒ‡ä»¤ç¤ºä¾‹ï¼š")
        gr.Markdown("- äººåƒï¼šè‡ªæ‹På¸…ç‚¹ | ç£¨çš®ç¾ç™½ | äº”å®˜ä¿®ç²¾è‡´ | æ”¾å¤§å¹¶ç¾é¢œ | å£ç½©ç…§ä¼˜åŒ–")
        gr.Markdown("- åŠ¨ç‰©ï¼šçŒ«å’ªè°ƒèŒç‚¹ | ç‹—ç‹—ç…§ç‰‡é”åŒ– | åŠ æ¸…æ–°æ—¥ç³»æ»¤é•œ")
        gr.Markdown("- æ™¯ç‰©ï¼šæµ·è¾¹åŠ ç”µå½±æ„Ÿ | å¤œæ™¯èµ›åšæœ‹å…‹ | å±±æ°´æ°´å¢¨é£ | ä»…è½»å¾®æ¸…æ™°")
        
        with gr.Row():
            with gr.Column(scale=1, min_width=320):
                input_image = gr.Image(
                    type="numpy", height=400, image_mode="RGB",
                    label="ä¸Šä¼ å›¾ç‰‡ï¼ˆæ”¯æŒé«˜æ¸…/ä½æ¸…/é®æŒ¡ç…§ï¼Œæœ€å¤§20MBï¼‰",
                    elem_id="upload-img"
                )
                user_prompt = gr.Textbox(
                    placeholder="è¾“å…¥ä¿®å›¾éœ€æ±‚ï¼ˆè‡ªç„¶è¯­è¨€å³å¯ï¼Œå¦‚ï¼šPå¥½çœ‹ç‚¹/å£ç½©ç…§ä¼˜åŒ–/åŠ ç”µå½±æ„Ÿï¼‰",
                    lines=4, label="ä¿®å›¾æŒ‡ä»¤", elem_id="prompt-input"
                )
                process_btn = gr.Button("âœ¨ å¼€å§‹æ™ºèƒ½ä¿®å›¾", variant="primary", size="lg")
            
            with gr.Column(scale=1, min_width=320):
                output_image = gr.Image(
                    height=400, image_mode="RGB",
                    label="ä¿®å›¾ç»“æœï¼ˆå¯ç›´æ¥ä¸‹è½½ï¼‰", elem_id="result-img"
                )
                result_text = gr.Textbox(
                    label="å¤„ç†æ—¥å¿—ï¼ˆæŸ¥çœ‹æµç¨‹/ç»“æœï¼‰", interactive=False, lines=6, elem_id="status-text"
                )
        
        # ç»‘å®šæŒ‰é’®ç‚¹å‡»äº‹ä»¶
        process_btn.click(
            fn=process_image,
            inputs=[input_image, user_prompt],
            outputs=[output_image, result_text],
            show_progress=True
        )
        
        # æœ¬åœ°ç•Œé¢ç¾åŒ–CSSï¼ˆé€‚é…ç”µè„‘ç«¯ï¼‰
        demo.css = """
        #upload-img, #result-img {border: 2px dashed #6366F1 !important; border-radius: 12px !important; margin-bottom: 15px !important;}
        #prompt-input {border-radius: 12px !important; border: 1px solid #C7D2FE !important; padding: 12px !important; margin-bottom: 15px !important;}
        #status-text {border-radius: 12px !important; border: 1px solid #C7D2FE !important; background: #F5F7FF !important; padding: 12px !important;}
        .gradio-container {max-width: 1200px !important; margin: 1rem auto !important; padding: 1.5rem !important;}
        button {border-radius: 12px !important; background: #6366F1 !important; color: white !important; border: none !important; font-size: 16px !important; padding: 10px 0 !important;}
        button:hover {background: #4F46E5 !important; transform: scale(1.02) !important; transition: all 0.2s ease !important;}
        h1 {color: #4F46E5 !important; text-align: center; margin-bottom: 1rem !important; font-weight: 700 !important;}
        h3, .markdown h4 {color: #6366F1 !important; margin: 0.8rem 0 !important;}
        .markdown li {margin: 0.4rem 0 !important; color: #4B5563 !important; line-height: 1.6 !important;}
        .gr-col {margin: 0 20px !important;}
        """
    
    # å¯åŠ¨æœ¬åœ°æœåŠ¡ï¼ˆé»˜è®¤7860ï¼Œè¢«å åˆ™è‡ªåŠ¨åˆ‡8080ï¼Œä»…æœ¬åœ°å¯è®¿é—®ï¼‰
    try:
        demo.launch(
            share=False,
            server_name="127.0.0.1",
            server_port=7860,
            show_error=True,
            quiet=True,
            max_file_size="20MB"
        )
    except OSError:
        demo.launch(
            share=False,
            server_name="127.0.0.1",
            server_port=8080,
            show_error=True,
            quiet=True,
            max_file_size="20MB"
        )